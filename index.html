<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Diffusion Guided Adaptive Augmentation for Generalization in Visual Reinforcement Learning </title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon/stablediffusison_photo_icon.ico">
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon/DALLE3_illust_icon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <!-- <style>
    table, th, td {
        border: 1px solid black;
        border-collapse: collapse;
    }
    th, td {
        padding: 10px;
    }
  </style> -->

  <style>
    table, th, td {
      border: 1px solid black;
      border-collapse: collapse;
    }
    .item table {
      margin-left: auto; 
      margin-right: auto; 
      display: block;
      width: 90%; /* 표의 너비를 70%로 설정 */
      font-size: 16px; /* 글자 크기를 조정 */
    }
    .item th, .item td {
      padding: 10px; /* 셀의 안쪽 여백을 넓힙니다. */
    }
    .thick-border {
      border-top: 3px solid black; /* 굵은 아래 경계 설정 */
      border-collapse: collapse;
    }
    .responsive-image {
      width: 600px;  /* 원하는 너비 */
      height: 300px; /* 원하는 높이 */
      object-fit: fill; /* 이미지를 비율 유지하며 잘림 방지 */
      margin: 0 auto; /* 중앙 정렬 */
    }    
    .three-image {
      width: 900px;  /* 원하는 너비 */
      height: 450px; /* 원하는 높이 */
      object-fit: fill; /* 이미지를 비율 유지하며 잘림 방지 */
      margin: 0 auto; /* 중앙 정렬 */
    }
  </style>
  <style>
    .tool_table, .tool_table th, .tool_table td {
      border: 2px solid black; /* 테두리 색상을 파란색으로 변경하고, 두께를 2px로 조정 */
      border-collapse: collapse;
    }
    .tool_table {
      margin: 20px auto; /* 상하 마진을 20px, 좌우 마진을 자동으로 설정하여 가운데 정렬 */
      display: block;
      width: 70%; /* 표의 너비를 50%로 변경 */
      background-color: white; /* 배경색을 연회색으로 설정 */
    }
    .tool_table th, .tool_table td {
      padding: 20px; /* 셀의 안쪽 여백을 20px로 넓힙니다. */
      text-align: left; /* 텍스트를 왼쪽 정렬합니다. */
      background-color: white; /* 헤더 배경색을 파란색으로 설정 */
      color: black; /* 헤더 글자 색상을 흰색으로 설정 */
    }
  </style>

  <style>
    .license-text {
      background-color: #D3D3D3; /* 이 부분에서 회색 배경색을 지정합니다. */
      padding: 15px; /* 이 부분에서 텍스트와 테두리 사이의 여백을 지정합니다. */
    }
    .second-image {
      display: flex;
      margin: auto;
      max-width: 300px; /* 원하는 크기로 설정 */
      max-height: 450px;
      justify-content: center; /* 중앙 정렬 */
      align-items: center; /* 중앙 정렬 */
      object-fit: fill; /* 이미지를 비율 유지하며 잘림 방지 */
    }
    .map-image {
      display: flex;
      margin: auto;
      width: 300px; /* 원하는 크기로 설정 */
      height: 450px;
      justify-content: center; /* 중앙 정렬 */
      align-items: center; /* 중앙 정렬 */
      object-fit: fill; /* 이미지를 비율 유지하며 잘림 방지 */
    }
  </style>

  <style>
    .gallery {
      display: flex;
      flex-direction: column;
      gap: 20px; /* 그룹 간의 간격 */
  }

  .group {
      background: #f9f9f9; /* 배경색 */
      padding: 20px; /* 여백 */
      border-radius: 10px; /* 모서리 둥글게 */
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1); /* 그림자 효과 */
      overflow-y: auto; /* 세로 스크롤 추가 */
      max-height: 300px; /* 최대 높이 설정 */
  }

  .group h2 {
      margin-bottom: 10px; /* 제목과 이미지 간의 간격 */
  }

  .image-item {
      display: inline-block; /* 이미지 항목을 인라인 블록으로 설정 */
      margin: 5px; /* 이미지 간의 간격 */
  }

  .image-item img {
      width: 100px; /* 이미지 너비 */
      height: auto; /* 비율 유지 */
      border-radius: 5px; /* 모서리 둥글게 */
  }

  </style>

</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Diffusion Guided Adaptive Augmentation for Generalization in Visual Reinforcement Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <!-- <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Jeong Woon Lee</a>,</span> -->
                Jeong Woon Lee and Hyoseok Hwang
                <br> {everyman, hyoseok}@khu.ac.uk
                <!-- <span class="author-block"> -->
                  <!-- <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Hyoseok Hwang</a>,</span> -->


                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <a href="http://airlab.khu.ac.kr/" target="_blank"> Kyung Hee University AIRLAB </a><br>ICCV 2025</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10679919" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block"> -->
                      <!-- <a href="static/pdfs/supplementary_material.pdf" target="_blank" -->
                      <!-- class="external-link button is-normal is-rounded is-dark"> -->
                      <!-- <span class="icon"> -->
                        <!-- <i class="fas fa-file-pdf"></i> -->
                      <!-- </span> -->
                      <!-- <span>Supplementary</span> -->
                    <!-- </a> -->
                  <!-- </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/AIRLABkhu/DGA2_Code" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                  </a> -->
              </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<style>
  .column h3 {
    font-size: 24px; /* 폰트 크기 조정 */
  }

  .column ul li {
    font-size: 18px; /* 폰트 크기 조정 */
  }

</style>


  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
Reinforcement learning (RL) has proven its potential in complex decision-making tasks.Yet, many RL systems rely on manually crafted state representations, requiring effort in feature engineering.Visual Reinforcement Learning (VRL) offers a way to address this challenge by enabling agents to learn directly from raw visual input. Nonetheless, VRL continues to face generalization issues, as models often overfit to specific domain features. To tackle this issue, we propose Diffusion Guided Adaptive Augmentation (DGA2), an augmentation method that utilizes Stable Diffusion to enhance domain diversity. We introduce an Adaptive Domain Shift strategy that dynamically adjusts the degree of domain shift according to the agent’s learning progress for effective augmentation with Stable Diffusion.Additionally, we employ saliency as the mask to preserve the semantics of data. Our experiments on the DMControl-GB, Adroit, and Procgen environments demonstrate that DGA2 improves generalization performance compared to existing data augmentation and generalization methods.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Method carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Method</h2>
        <div id="results-carousel" class="carousel results-carousel" style="text-align: center;">

          <!-- Adaptive Domain Shift -->
         <div class="item" style="height: 100%; display: flex; flex-direction: column; justify-content: space-between; align-items: center;"> 
          <img src="static/images/adaptive_domain_shift_main.png" alt="fail">
          <h2 class="subtitle has-text-left" style="padding-bottom: 1%; font-size: 12pt">
            <b>Figure 1.</b> Adaptive Domain Shift (ADS) adjusts the extent of domain shift based on episode return changes. If the episode return improves, ADS shifts the learning domain according to the change in episode return. Otherwise, the agent continues learning in the previous domain.
          </h2>
        </div>

        <!-- Overview -->
        <div class="item"> 
          <img src="static/images/architecture.png" alt="fail" >
          <h2 class="subtitle has-text-left" style="padding-bottom: 1%; font-size: 12pt">
            <b>Figure 2.</b>  The Adaptive Domain Shift module dynamically modulates domain variations based on the agent’s episode return. The Sample Generation module integrates saliency maps extracted from the agent’s value network to preserve task-relevant regions while augmenting background domains with those generated by Stable Diffusion.
          </h2>
        </div>


    </div>
  </div>
  </div>
  </section>
  <!-- End method carousel -->


  <!-- Quantitative Evaluation -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Quantitative Evaluation</h2>

        <h5 class="title is-5">Main Results</h3> 
        <p style="margin-bottom:2em;">
          The evaluation was conducted in DMControl-GB, Procgen, and Adroit to assess the effectiveness of DGA2. The results demonstrated that our method boost the generalization capability of baseline on various domain and environments.
        </p>

        <div id="results-carousel" class="carousel results-carousel" style="text-align: center;">

          <!-- DMControl-GB -->
         <div class="item"> 
          <img src="static/images/dmcontrol.png" alt="fail">
          <h2 class="subtitle has-text-left" style="padding-bottom: 1%; font-size: 12pt">
            <b>Table 1.</b> Comparison with other methods in DMControl-GB after training on 500K environment steps. We provide the mean and standard deviation of episode return repeated three times with different random seeds. (·) represents the standard deviation.
          </h2>
        </div>

        <!-- Procgen -->
        <div class="item"> 
          <img src="static/images/procgen.png" alt="fail" >
          <h2 class="subtitle has-text-left" style="padding-bottom: 1%; font-size: 12pt">
            <b>Table 2.</b>  Comparison with other methods in Procgen after training on 25M environment steps. We provide the mean and standard deviation of episode return trained with three different random seeds. (·) represents the standard deviation.
          </h2>
        </div>

        <!-- Adroit -->
        <div class="item"> 
          <img src="static/images/adroit.png" alt="fail" >
          <h2 class="subtitle has-text-left" style="padding-bottom: 1%; font-size: 12pt">
            <b>Table 3.</b>  Comparison with other methods in Adroit after training on 500K environment steps. We provide the mean and standard deviation of success rate trained with three different random seeds. (·) represents the standard deviation.
          </h2>
        </div>

    </div>
    <h5 class="title is-5">Analysis on Adaptive Domain Shift</h3> 
    <p style="margin-bottom:2em;">
    We evaluated the effectiveness of the Adaptive Domain Shift (ADS) strategy by comparing four methods: training withing an identical domain (ID), domain switching at fixed steps (DS(S)), domain switching upon episode termination (DS(E)), and progressively changing domains at the end of each episode(CD).
    In Table 4. the results suggested that adaptively adjusting the domain based on episode return was more effective than other strategies.
    </p>
  <figure style="text-align: center; margin-bottom: 1em;">
    <img src="static/images/adaptive_domain_shift.png" alt="fail" width="48%">
    <figcaption class="content has-text-centered" style="word-break:normal">
      <b>Table 4.</b> Experimental results on difference domain shift strategies.
    </figcaption>
  </figure>

  <h5 class="title is-5">Analysis on Sample Efficiency</h3> 
  <p style="margin-bottom:2em;">
We compareed test performance across environment steps, as shown in Figure 3. 
Although DGA2 is activated only after 100K steps, it rapidly outperforms all baseline methods and achieves the highest performance by 200K steps. 
This indicates that our method requires significantly fewer environment interactions to reach a given performance level.
  </p>
<figure style="text-align: center; margin-bottom: 1em;">
  <img src="static/images/sample_efficiency.png" alt="fail" width="48%">
  <figcaption class="content has-text-centered" style="word-break:normal">
    <b>Figure 3.</b> Test performance over environment steps.
  </figcaption>
  </figure>

  <!-- Qualitative Evaluation -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Qualitative Evaluation</h2>
  <h5 class="title is-5">Visualization of Saliency Map</h3> 
  <p style="margin-bottom:2em;">
We assessed generalization capability using saliency maps in unseed domains. 
As shown in Figure 4, NoAug often attended to irrelevant backgrounds, failing to identify task-relevant areas in Ball in cup catch (Video Hard).
Overlay improved attention but exhibited similar tendencies with NoAug, especially in Walker walk.
In contrast, our method consistently focused on task-relevant areas across both Color Hard and Video Hard.
  </p>
<figure style="text-align: center; margin-bottom: 1em;">
  <img src="static/images/qualitative_analysis.png" alt="fail" width="48%">
  <figcaption class="content has-text-centered" style="word-break:normal">
    <b>Figure 4.</b> Visualization of saliency map of Walker walk and Ball in cup catch in Color Hard and Video Hard. (a) Original Image, Saliency Map of (b) NoAug, (c) Overlay, and (d) DGA2.
  </figcaption>
  </figure>


  <h5 class="title is-5">Visualization of Augmented Samples</h3> 
  <p style="margin-bottom:2em;">
We visualized the augmented images and selected SRM and Overlay as comparisons to observe pixel-level changes alongside our method.
As depicted in Figure 5, our method, similar to Overlay, was capable of generating a diverse of domains.
Additionally, by integrating saliency, our approach highlighted and preserved task-relevant regions.
  </p>
<figure style="text-align: center; margin-bottom: 1em;">
  <img src="static/images/qualitative_analysis2.png" alt="fail" width="48%">
  <figcaption class="content has-text-centered" style="word-break:normal">
    <b>Figure 5.</b> Visualization of augmented images. (a) Original images, and augmented images from (b) SRM, (c) Overlay, and (d) DGA2.
  </figcaption>
  </figure>
        
<!--BibTex citation -->
<!--   <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @ARTICLE{10679919,
        author={Kim, Geunu and Kim, Daeho and Jang, Jaeyun and Hwang, Hyoseok},
        journal={IEEE Robotics and Automation Letters}, 
        title={PAIR360: A Paired Dataset of High-Resolution 360${}^{\circ }$ Panoramic Images and LiDAR Scans}, 
        year={2024},
        volume={},
        number={},
        pages={1-8},
        keywords={Cameras;Laser radar;Sensors;Robot vision systems;Autonomous vehicles;Global Positioning System;Synchronization;Data Sets for SLAM;Sensor Fusion;Omnidirectional Vision;Data Sets for Robotic Vision},
        doi={10.1109/LRA.2024.3460418}}</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->



  </body>
  </html>
